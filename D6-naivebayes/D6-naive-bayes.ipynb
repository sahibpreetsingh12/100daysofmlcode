{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liked-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why we are ignoring Marginal prob term ( denominator) of Naive bayes\n",
    "# https://chrisalbon.com/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "junior-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    '''\n",
    "    Bayes Theorem form\n",
    "    P(y|X) = P(X|y) * P(y) / P(X)\n",
    "    '''\n",
    "    def calc_prior(self, features, target):\n",
    "        '''\n",
    "        prior probability P(y)\n",
    "        calculate prior probabilities\n",
    "        '''\n",
    "        self.prior = (features.groupby(target).apply(lambda x: len(x)) / self.rows).to_numpy()\n",
    "\n",
    "        return self.prior\n",
    "    \n",
    "    def calc_statistics(self, features, target):\n",
    "        '''\n",
    "        calculate mean, variance for each column and convert to numpy array\n",
    "        ''' \n",
    "        self.mean = features.groupby(target).apply(np.mean).to_numpy()\n",
    "        self.var = features.groupby(target).apply(np.var).to_numpy()\n",
    "              \n",
    "        return self.mean, self.var\n",
    "    \n",
    "    def gaussian_density(self, class_idx, x):     \n",
    "        '''\n",
    "        calculate probability from gaussian density function (normally distributed)\n",
    "        we will assume that probability of specific target value given specific class is normally distributed \n",
    "        \n",
    "        probability density function for gaussain dist. is:\n",
    "        (1/√2pi*σ) * exp((-1/2)*((x-μ)^2)/(2*σ²)), where μ is mean, σ² is variance, σ is quare root of variance (standard deviation)\n",
    "        '''\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx]\n",
    "        numerator = np.exp((-1/2)*((x-mean)**2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        prob = numerator / denominator\n",
    "        return prob\n",
    "    \n",
    "    def calc_posterior(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # calculate posterior probability for each class\n",
    "        for i in range(self.count):\n",
    "            prior = self.prior[i] ## use the log to make it more numerically stable\n",
    "            conditional = np.sum(self.gaussian_density(i, x)) # use the log to make it more numerically stable\n",
    "            posterior = prior * conditional\n",
    "            posteriors.append(posterior)\n",
    "        # return class with highest posterior probability\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "    def fit(self, features, target):\n",
    "        self.classes = np.unique(target)\n",
    "        self.count = len(self.classes)\n",
    "        self.feature_nums = features.shape[1]\n",
    "        self.rows = features.shape[0]\n",
    "        \n",
    "        self.calc_statistics(features, target)\n",
    "        self.calc_prior(features, target)\n",
    "        \n",
    "    def predict(self, features):\n",
    "        preds = [self.calc_posterior(f) for f in features.to_numpy()]\n",
    "        return preds\n",
    "\n",
    "    def accuracy(self, y_test, y_pred):\n",
    "        accuracy = np.sum(y_test == y_pred) / len(y_test)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alternative-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "(100, 4) (100,)\n",
      "(50, 4) (50,)\n"
     ]
    }
   ],
   "source": [
    "# upload Iris dataset -  shape is (150, 5)\n",
    "df = pd.read_csv(\"/home/sahib/100days/D6-naivebayes/iris.csv\")\n",
    "# shuffle dataset with sample\n",
    "df = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "# df shape\n",
    "print(df.shape)\n",
    "# set features and target\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "\n",
    "# # split on train and test 0.7/0.3\n",
    "X_train, X_test, y_train, y_test = X[:100], X[100:], y[:100], y[100:]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "champion-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "x = NaiveBayesClassifier()\n",
    "\n",
    "\n",
    "x.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sixth-allah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = x.predict(X_test)\n",
    "x.accuracy(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

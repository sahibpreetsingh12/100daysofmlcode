{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fitted-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading data and importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "organic-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(): # Multilayer Perceptron\n",
    "    def __init__(self,epochs,lr):\n",
    "        self.learning_rate = lr\n",
    "        self.epochs=epochs\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x)) \n",
    "    def der_sigmoid(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "    def fit(self,features,labels):\n",
    "        weights_layer1 = np.random.rand(features.shape[1],features.shape[1]*2) # weight's shape (2,4)  \n",
    "        weights_layer2 = np.random.rand(features.shape[1]*2,6) # weights for hidden layer1 to hidden layer 2\n",
    "        weights_layer3 = np.random.rand(6,1) # hidden layer having 6 neurons and going to 1 neuron for output\n",
    "        for i in range(self.epochs):\n",
    "            ls=[]\n",
    "             # feedforward\n",
    "            \n",
    "            # layer 1 --> layer 2  ( 2 input neurons to 4 neurons in hidden layer1)\n",
    "            z1 = np.dot(features, weights_layer1)\n",
    "            a1 = self.sigmoid(z1)\n",
    "            \n",
    "            # layer 2 ---> layer 3 ( 4 neurons in hidden layer 1 to 6 neurons in hidden layer 2 )\n",
    "            z2 = np.dot(a1, weights_layer2)\n",
    "            a2 = self.sigmoid(z2)\n",
    "            \n",
    "            # layer 3 --> output layer ( 6 neurons in hidden layer 2 to 1 neuron in output layer)\n",
    "            z3 = np.dot(a2,weights_layer3)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            ls.append(a3)\n",
    "            # backpropagation\n",
    "            labels = labels.reshape(-1,1)\n",
    "            # Phase 1 # Updating weights of hidden layer 2 (hidden layer2 to output layer)\n",
    "            error_out = ((1 / 2) * (np.power((a3 - labels), 2)))\n",
    "            dcost_dao = 2*(a3 - labels) # derivative of cost wrt ao\n",
    "            dao_dzo = self.der_sigmoid(z3) # derivative of ao wrt to zo\n",
    "            dzo_dwo = a2 # derivative of zo wrt w_3\n",
    "            \n",
    "            dcost_weight_output_layer = np.dot(dzo_dwo.T, dcost_dao * dao_dzo)\n",
    "            weights_layer3 -= self.learning_rate * dcost_weight_output_layer\n",
    "            \n",
    "            # Phase 2 # Updating weights of hidden layer 1 ( hidden layer1 to hidden layer 2 )\n",
    "            #dcost_dw2 = dcost_da2 * da2_dz2 * dz2_dw2\n",
    "            # Breaking ------->  dcost_da2\n",
    "            # dcost_da2 = dcost_dao *dao_dzo *dzo_da2 \n",
    "            \n",
    "            #Final quation\n",
    "            # dcost_dw2 = (dcost_dao *dao_dzo) * (dzo_da2 .T  * da2_dz2 .T ) * (dz2_dw2) === >  (part1) * (part2) *(dz2_dw2)\n",
    "            \n",
    "            dzo_da2 = weights_layer3.T # 1,6\n",
    "            \n",
    "            da2_dz2 = self.der_sigmoid(z2).T # 6,100\n",
    "            dz2_dw2 = a1 #100,4\n",
    "            part1 = np.dot(dcost_dao.T,dao_dzo) #1 , 1 \n",
    "            \n",
    "            part2 = np.dot(dzo_da2,da2_dz2) # 1,100\n",
    "\n",
    "            temp = np.dot(part1,part2) # 1,100\n",
    "            dcost_dw2=np.dot(temp,dz2_dw2).T \n",
    "            weights_layer2 -= self.learning_rate * dcost_dw2\n",
    "            \n",
    "            # Phase 3 # Updating weights of Input layer ( input layer to hidden layer1 )\n",
    "            \n",
    "            # dcost_dw1 = dcost_a1 * da1_dz1 * dz1_dw1\n",
    "\n",
    "            # Breaking ---> dcost_da1 = dcost_dzo * dzo_da1 ( means dcost_da1 consists of 2 parts )= part1 * part2\n",
    "            # Breaking part1 ---> dcost_dzo = dcost_dao * dao_dzo \n",
    "            # Breaking part2 ----> dzo_da1 = dzo_da2 * da2_dz2 * dz2_da1 \n",
    "            # part1 is already calculated above ( dcost_dzo = dcost_dao * dao_dzo  ) # 1,1\n",
    "            dzo_da2 = weights_layer3.T # 1,6\n",
    "        \n",
    "#             # da2_dz2 already calulate above # 6,100\n",
    "#             print(\"da2_dz2\",da2_dz2.shape)\n",
    "            dz2_da1 = weights_layer2.T # 6,4\n",
    "            part2 = np.dot(dzo_da2,dz2_da1)\n",
    "        \n",
    "            temp= np.dot(part1,part2)\n",
    "    \n",
    "            da1_dz1 = self.der_sigmoid(z1).T # 4,100\n",
    "            \n",
    "            dz1_dw1 = features # 100,2\n",
    "            \n",
    "            part3 = np.dot(da1_dz1,dz1_dw1)\n",
    "            \n",
    "            dcost_dw1 = np.dot(temp,part3).T # (2,1)\n",
    "            # final eq\n",
    "            weights_layer1 -= self.learning_rate * dcost_dw1\n",
    "        return ls # predictions\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "annual-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-193-c375d3a7c125>:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [4.81937919e-13],\n",
       "        [6.94217005e-07],\n",
       "        [6.94217005e-07],\n",
       "        [4.81937919e-13]])]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s =MLP(2000,0.9)\n",
    "features,target = datasets.make_moons(100, noise=0.25)\n",
    "prediction=s.fit(features,target)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python385jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
